{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSGY9HUDim6E"
      },
      "source": [
        "Access to google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eaPtK19ibOu",
        "outputId": "409ec48e-690e-4362-aefc-65e19597494b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ2NUYD1isfA"
      },
      "source": [
        "- Define dataset class\n",
        "- define model architecture (cnn)\n",
        "- run training loop & save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uf4fm5FRi3Wm"
      },
      "outputs": [],
      "source": [
        "# import libs\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCKkeZjVjJp5",
        "outputId": "15e99805-d37f-427e-d50c-9217b1cd17a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using cuda\n"
          ]
        }
      ],
      "source": [
        "# config\n",
        "DRIVE_BASE_PATH = \"/content/drive/MyDrive/chess_games/\"\n",
        "\n",
        "TRAIN_FILES = [\n",
        "    os.path.join(DRIVE_BASE_PATH, \"stockfished_dataset_carlsen.pt\"),\n",
        "    os.path.join(DRIVE_BASE_PATH, \"stockfished_dataset_caruana.pt\"),\n",
        "    os.path.join(DRIVE_BASE_PATH, \"stockfished_dataset_firouzja.pt\"),\n",
        "    os.path.join(DRIVE_BASE_PATH, \"stockfished_dataset_karpov.pt\")\n",
        "]\n",
        "\n",
        "VALID_FILE = os.path.join(DRIVE_BASE_PATH, \"stockfished_dataset_kasparov.pt\")\n",
        "\n",
        "MODEL_SAVE_PATH = os.path.join(DRIVE_BASE_PATH, \"sf_chess_model.pth\")\n",
        "\n",
        "#hyperparameters\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = .0001\n",
        "\n",
        "# device and check gpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"using {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "r671DVxCj1tF"
      },
      "outputs": [],
      "source": [
        "# dataset class\n",
        "class ChessDataset(Dataset):\n",
        "  def __init__(self, file_paths):\n",
        "    print(\"loading dataset... takes time\")\n",
        "\n",
        "    self.X = [] # board tensor list\n",
        "    self.y = [] # label tensor list\n",
        "    total_positions = 0\n",
        "    for file_path in file_paths:\n",
        "      try:\n",
        "        print(f\"loading file {file_path}\")\n",
        "        data = torch.load(file_path)\n",
        "        x_data = data[0]\n",
        "        y_data = data[1]\n",
        "\n",
        "        self.X.extend(x_data)\n",
        "        self.y.extend(y_data)\n",
        "\n",
        "        num_loaded = len(y_data)\n",
        "        total_positions += num_loaded\n",
        "        print(f\"dataset loaded, {num_loaded} positions.\")\n",
        "\n",
        "        del data\n",
        "        del x_data\n",
        "        del y_data\n",
        "      except FileNotFoundError:\n",
        "        print(f\"ERROR! no file {file_path}\")\n",
        "      except Exception as e:\n",
        "        print(f\"ERROR! loading dataset: {e}\")\n",
        "        print(\"check for out of memory\")\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.y)\n",
        "\n",
        "  def __getitem__(self, idx): # stack tensors into a single tensor\n",
        "    return self.X[idx], self.y[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UfQfcAK2kx2v"
      },
      "outputs": [],
      "source": [
        "# model architecture (cnn)\n",
        "class ChessCNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ChessCNN, self).__init__() # input (batch_size, 12, 8, 8)\n",
        "\n",
        "    # convolutional block 1\n",
        "    self.conv1 = nn.Conv2d(12, 32, kernel_size=3, padding=1) # (batch, 32, 8, 8)\n",
        "    self.bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "    # convolutional block 2\n",
        "    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) # (batch, 64, 8, 8)\n",
        "    self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "    # convolutional block 3\n",
        "    self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # (batch, 128, 8, 8)\n",
        "    self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "    # connected layers\n",
        "    self.flatten = nn.Flatten()\n",
        "    # 128 chn * 8 * 8 = 8192\n",
        "    self.fc1 = nn.Linear(128 * 8 * 8, 512)\n",
        "    self.dropout = nn.Dropout(.5)\n",
        "    self.fc2 = nn.Linear(512, 1) # output single score\n",
        "\n",
        "    # activation functions\n",
        "    self.relu = nn.ReLU()\n",
        "    self.tanh = nn.Tanh()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # apply conv blocks\n",
        "    x = self.relu(self.bn1(self.conv1(x)))\n",
        "    x = self.relu(self.bn2(self.conv2(x)))\n",
        "    x = self.relu(self.bn3(self.conv3(x)))\n",
        "\n",
        "    x = self.flatten(x)\n",
        "\n",
        "    # apply fc layers\n",
        "    x = self.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    # output activation\n",
        "    x = self.tanh(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLoRb5f_vOf_"
      },
      "source": [
        "Loading Dataset & Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g4OzTu6vPwo",
        "outputId": "d59d002a-a37a-4720-f8d9-6c508f6036a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading train data...\n",
            "loading dataset... takes time\n",
            "loading file /content/drive/MyDrive/chess_games/stockfished_dataset_carlsen.pt\n",
            "dataset loaded, 653423 positions.\n",
            "loading file /content/drive/MyDrive/chess_games/stockfished_dataset_caruana.pt\n",
            "dataset loaded, 550019 positions.\n",
            "loading file /content/drive/MyDrive/chess_games/stockfished_dataset_firouzja.pt\n",
            "dataset loaded, 436867 positions.\n",
            "loading file /content/drive/MyDrive/chess_games/stockfished_dataset_karpov.pt\n",
            "dataset loaded, 294121 positions.\n",
            "\n",
            "loading valid data...\n",
            "loading dataset... takes time\n",
            "loading file /content/drive/MyDrive/chess_games/stockfished_dataset_kasparov.pt\n",
            "dataset loaded, 161951 positions.\n",
            "\n",
            "dataset ready to train\n",
            "\n",
            "===== TRAINING STARTS\n",
            "[epoch 1, batch  1000] loss 0.1225\n",
            "[epoch 1, batch  2000] loss 0.1218\n",
            "[epoch 1, batch  3000] loss 0.1212\n",
            "[epoch 1, batch  4000] loss 0.1208\n",
            "[epoch 1, batch  5000] loss 0.1208\n",
            "[epoch 1, batch  6000] loss 0.1209\n",
            "[epoch 1, batch  7000] loss 0.1208\n",
            "[epoch 1, batch  8000] loss 0.1208\n",
            "[epoch 1, batch  9000] loss 0.1207\n",
            "[epoch 1, batch 10000] loss 0.1206\n",
            "[epoch 1, batch 11000] loss 0.1206\n",
            "[epoch 1, batch 12000] loss 0.1206\n",
            "[epoch 1, batch 13000] loss 0.1205\n",
            "[epoch 1, batch 14000] loss 0.1204\n",
            "[epoch 1, batch 15000] loss 0.1204\n",
            ">>> epoch 1 finished in 78.11 seconds...\n",
            "\n",
            "avg training loss: 0.1204\n",
            "\n",
            "avg valid loss: 0.1118\n",
            "learning rate: [0.0001]\n",
            "valid loss improved, saving model /content/drive/MyDrive/chess_games/sf_chess_model.pth\n",
            "[epoch 2, batch  1000] loss 0.1206\n",
            "[epoch 2, batch  2000] loss 0.1202\n",
            "[epoch 2, batch  3000] loss 0.1205\n",
            "[epoch 2, batch  4000] loss 0.1201\n",
            "[epoch 2, batch  5000] loss 0.1199\n",
            "[epoch 2, batch  6000] loss 0.1199\n",
            "[epoch 2, batch  7000] loss 0.1199\n",
            "[epoch 2, batch  8000] loss 0.1199\n",
            "[epoch 2, batch  9000] loss 0.1199\n",
            "[epoch 2, batch 10000] loss 0.1200\n",
            "[epoch 2, batch 11000] loss 0.1199\n",
            "[epoch 2, batch 12000] loss 0.1199\n",
            "[epoch 2, batch 13000] loss 0.1200\n",
            "[epoch 2, batch 14000] loss 0.1200\n",
            "[epoch 2, batch 15000] loss 0.1200\n",
            ">>> epoch 2 finished in 76.11 seconds...\n",
            "\n",
            "avg training loss: 0.1200\n",
            "\n",
            "avg valid loss: 0.1113\n",
            "learning rate: [0.0001]\n",
            "valid loss improved, saving model /content/drive/MyDrive/chess_games/sf_chess_model.pth\n",
            "[epoch 3, batch  1000] loss 0.1197\n",
            "[epoch 3, batch  2000] loss 0.1196\n",
            "[epoch 3, batch  3000] loss 0.1200\n",
            "[epoch 3, batch  4000] loss 0.1199\n",
            "[epoch 3, batch  5000] loss 0.1199\n",
            "[epoch 3, batch  6000] loss 0.1198\n",
            "[epoch 3, batch  7000] loss 0.1198\n",
            "[epoch 3, batch  8000] loss 0.1198\n",
            "[epoch 3, batch  9000] loss 0.1198\n",
            "[epoch 3, batch 10000] loss 0.1199\n",
            "[epoch 3, batch 11000] loss 0.1199\n",
            "[epoch 3, batch 12000] loss 0.1199\n",
            "[epoch 3, batch 13000] loss 0.1198\n",
            "[epoch 3, batch 14000] loss 0.1199\n",
            "[epoch 3, batch 15000] loss 0.1198\n",
            ">>> epoch 3 finished in 76.06 seconds...\n",
            "\n",
            "avg training loss: 0.1198\n",
            "\n",
            "avg valid loss: 0.1111\n",
            "learning rate: [0.0001]\n",
            "valid loss improved, saving model /content/drive/MyDrive/chess_games/sf_chess_model.pth\n",
            "[epoch 4, batch  1000] loss 0.1193\n",
            "[epoch 4, batch  2000] loss 0.1194\n",
            "[epoch 4, batch  3000] loss 0.1191\n",
            "[epoch 4, batch  4000] loss 0.1193\n",
            "[epoch 4, batch  5000] loss 0.1194\n",
            "[epoch 4, batch  6000] loss 0.1194\n",
            "[epoch 4, batch  7000] loss 0.1195\n",
            "[epoch 4, batch  8000] loss 0.1196\n",
            "[epoch 4, batch  9000] loss 0.1195\n",
            "[epoch 4, batch 10000] loss 0.1195\n",
            "[epoch 4, batch 11000] loss 0.1195\n",
            "[epoch 4, batch 12000] loss 0.1195\n",
            "[epoch 4, batch 13000] loss 0.1195\n",
            "[epoch 4, batch 14000] loss 0.1195\n",
            "[epoch 4, batch 15000] loss 0.1195\n",
            ">>> epoch 4 finished in 76.75 seconds...\n",
            "\n",
            "avg training loss: 0.1195\n",
            "\n",
            "avg valid loss: 0.1103\n",
            "learning rate: [0.0001]\n",
            "valid loss improved, saving model /content/drive/MyDrive/chess_games/sf_chess_model.pth\n",
            "[epoch 5, batch  1000] loss 0.1193\n",
            "[epoch 5, batch  2000] loss 0.1191\n",
            "[epoch 5, batch  3000] loss 0.1194\n",
            "[epoch 5, batch  4000] loss 0.1194\n",
            "[epoch 5, batch  5000] loss 0.1194\n",
            "[epoch 5, batch  6000] loss 0.1193\n",
            "[epoch 5, batch  7000] loss 0.1192\n",
            "[epoch 5, batch  8000] loss 0.1191\n",
            "[epoch 5, batch  9000] loss 0.1190\n",
            "[epoch 5, batch 10000] loss 0.1191\n",
            "[epoch 5, batch 11000] loss 0.1192\n",
            "[epoch 5, batch 12000] loss 0.1192\n",
            "[epoch 5, batch 13000] loss 0.1192\n",
            "[epoch 5, batch 14000] loss 0.1192\n",
            "[epoch 5, batch 15000] loss 0.1192\n",
            ">>> epoch 5 finished in 76.64 seconds...\n",
            "\n",
            "avg training loss: 0.1192\n",
            "\n",
            "avg valid loss: 0.1096\n",
            "learning rate: [0.0001]\n",
            "valid loss improved, saving model /content/drive/MyDrive/chess_games/sf_chess_model.pth\n",
            "[epoch 6, batch  1000] loss 0.1203\n",
            "[epoch 6, batch  2000] loss 0.1194\n",
            "[epoch 6, batch  3000] loss 0.1193\n",
            "[epoch 6, batch  4000] loss 0.1193\n",
            "[epoch 6, batch  5000] loss 0.1191\n",
            "[epoch 6, batch  6000] loss 0.1190\n",
            "[epoch 6, batch  7000] loss 0.1189\n",
            "[epoch 6, batch  8000] loss 0.1189\n",
            "[epoch 6, batch  9000] loss 0.1188\n",
            "[epoch 6, batch 10000] loss 0.1188\n",
            "[epoch 6, batch 11000] loss 0.1188\n",
            "[epoch 6, batch 12000] loss 0.1188\n",
            "[epoch 6, batch 13000] loss 0.1188\n",
            "[epoch 6, batch 14000] loss 0.1188\n",
            "[epoch 6, batch 15000] loss 0.1188\n",
            ">>> epoch 6 finished in 76.61 seconds...\n",
            "\n",
            "avg training loss: 0.1188\n",
            "\n",
            "avg valid loss: 0.1093\n",
            "learning rate: [0.0001]\n",
            "valid loss improved, saving model /content/drive/MyDrive/chess_games/sf_chess_model.pth\n",
            "[epoch 7, batch  1000] loss 0.1190\n",
            "[epoch 7, batch  2000] loss 0.1185\n",
            "[epoch 7, batch  3000] loss 0.1183\n",
            "[epoch 7, batch  4000] loss 0.1185\n",
            "[epoch 7, batch  5000] loss 0.1185\n",
            "[epoch 7, batch  6000] loss 0.1184\n",
            "[epoch 7, batch  7000] loss 0.1185\n",
            "[epoch 7, batch  8000] loss 0.1185\n",
            "[epoch 7, batch  9000] loss 0.1184\n",
            "[epoch 7, batch 10000] loss 0.1184\n",
            "[epoch 7, batch 11000] loss 0.1183\n",
            "[epoch 7, batch 12000] loss 0.1185\n",
            "[epoch 7, batch 13000] loss 0.1185\n",
            "[epoch 7, batch 14000] loss 0.1185\n",
            "[epoch 7, batch 15000] loss 0.1185\n",
            ">>> epoch 7 finished in 75.92 seconds...\n",
            "\n",
            "avg training loss: 0.1185\n",
            "\n",
            "avg valid loss: 0.1090\n",
            "learning rate: [0.0001]\n",
            "valid loss improved, saving model /content/drive/MyDrive/chess_games/sf_chess_model.pth\n",
            "[epoch 8, batch  1000] loss 0.1181\n",
            "[epoch 8, batch  2000] loss 0.1181\n",
            "[epoch 8, batch  3000] loss 0.1182\n",
            "[epoch 8, batch  4000] loss 0.1181\n",
            "[epoch 8, batch  5000] loss 0.1180\n",
            "[epoch 8, batch  6000] loss 0.1181\n",
            "[epoch 8, batch  7000] loss 0.1182\n",
            "[epoch 8, batch  8000] loss 0.1181\n",
            "[epoch 8, batch  9000] loss 0.1182\n",
            "[epoch 8, batch 10000] loss 0.1183\n",
            "[epoch 8, batch 11000] loss 0.1183\n",
            "[epoch 8, batch 12000] loss 0.1183\n",
            "[epoch 8, batch 13000] loss 0.1183\n",
            "[epoch 8, batch 14000] loss 0.1183\n",
            "[epoch 8, batch 15000] loss 0.1183\n",
            ">>> epoch 8 finished in 75.81 seconds...\n",
            "\n",
            "avg training loss: 0.1183\n",
            "\n",
            "avg valid loss: 0.1083\n",
            "learning rate: [0.0001]\n",
            "valid loss improved, saving model /content/drive/MyDrive/chess_games/sf_chess_model.pth\n",
            "[epoch 9, batch  1000] loss 0.1187\n",
            "[epoch 9, batch  2000] loss 0.1177\n",
            "[epoch 9, batch  3000] loss 0.1177\n",
            "[epoch 9, batch  4000] loss 0.1180\n",
            "[epoch 9, batch  5000] loss 0.1180\n",
            "[epoch 9, batch  6000] loss 0.1180\n",
            "[epoch 9, batch  7000] loss 0.1181\n",
            "[epoch 9, batch  8000] loss 0.1181\n",
            "[epoch 9, batch  9000] loss 0.1181\n",
            "[epoch 9, batch 10000] loss 0.1180\n",
            "[epoch 9, batch 11000] loss 0.1180\n",
            "[epoch 9, batch 12000] loss 0.1180\n",
            "[epoch 9, batch 13000] loss 0.1181\n",
            "[epoch 9, batch 14000] loss 0.1181\n",
            "[epoch 9, batch 15000] loss 0.1181\n",
            ">>> epoch 9 finished in 75.90 seconds...\n",
            "\n",
            "avg training loss: 0.1180\n",
            "\n",
            "avg valid loss: 0.1083\n",
            "learning rate: [0.0001]\n",
            "valid loss improved, saving model /content/drive/MyDrive/chess_games/sf_chess_model.pth\n",
            "[epoch 10, batch  1000] loss 0.1180\n",
            "[epoch 10, batch  2000] loss 0.1178\n",
            "[epoch 10, batch  3000] loss 0.1177\n",
            "[epoch 10, batch  4000] loss 0.1177\n",
            "[epoch 10, batch  5000] loss 0.1175\n",
            "[epoch 10, batch  6000] loss 0.1174\n",
            "[epoch 10, batch  7000] loss 0.1174\n",
            "[epoch 10, batch  8000] loss 0.1174\n",
            "[epoch 10, batch  9000] loss 0.1174\n",
            "[epoch 10, batch 10000] loss 0.1175\n",
            "[epoch 10, batch 11000] loss 0.1176\n",
            "[epoch 10, batch 12000] loss 0.1176\n",
            "[epoch 10, batch 13000] loss 0.1176\n",
            "[epoch 10, batch 14000] loss 0.1177\n",
            "[epoch 10, batch 15000] loss 0.1177\n",
            ">>> epoch 10 finished in 75.84 seconds...\n",
            "\n",
            "avg training loss: 0.1177\n",
            "\n",
            "avg valid loss: 0.1082\n",
            "learning rate: [0.0001]\n",
            "valid loss improved, saving model /content/drive/MyDrive/chess_games/sf_chess_model.pth\n",
            "[epoch 11, batch  1000] loss 0.1174\n",
            "[epoch 11, batch  2000] loss 0.1174\n",
            "[epoch 11, batch  3000] loss 0.1175\n",
            "[epoch 11, batch  4000] loss 0.1174\n",
            "[epoch 11, batch  5000] loss 0.1173\n",
            "[epoch 11, batch  6000] loss 0.1174\n",
            "[epoch 11, batch  7000] loss 0.1174\n",
            "[epoch 11, batch  8000] loss 0.1175\n",
            "[epoch 11, batch  9000] loss 0.1175\n",
            "[epoch 11, batch 10000] loss 0.1175\n",
            "[epoch 11, batch 11000] loss 0.1175\n",
            "[epoch 11, batch 12000] loss 0.1175\n",
            "[epoch 11, batch 13000] loss 0.1174\n",
            "[epoch 11, batch 14000] loss 0.1174\n",
            "[epoch 11, batch 15000] loss 0.1174\n",
            ">>> epoch 11 finished in 75.73 seconds...\n",
            "\n",
            "avg training loss: 0.1174\n",
            "\n",
            "avg valid loss: 0.1077\n",
            "learning rate: [0.0001]\n",
            "valid loss improved, saving model /content/drive/MyDrive/chess_games/sf_chess_model.pth\n",
            "[epoch 12, batch  1000] loss 0.1170\n",
            "[epoch 12, batch  2000] loss 0.1168\n",
            "[epoch 12, batch  3000] loss 0.1168\n",
            "[epoch 12, batch  4000] loss 0.1169\n",
            "[epoch 12, batch  5000] loss 0.1170\n",
            "[epoch 12, batch  6000] loss 0.1170\n",
            "[epoch 12, batch  7000] loss 0.1171\n",
            "[epoch 12, batch  8000] loss 0.1172\n",
            "[epoch 12, batch  9000] loss 0.1173\n",
            "[epoch 12, batch 10000] loss 0.1173\n",
            "[epoch 12, batch 11000] loss 0.1173\n",
            "[epoch 12, batch 12000] loss 0.1174\n",
            "[epoch 12, batch 13000] loss 0.1173\n",
            "[epoch 12, batch 14000] loss 0.1173\n",
            "[epoch 12, batch 15000] loss 0.1172\n",
            ">>> epoch 12 finished in 75.55 seconds...\n",
            "\n",
            "avg training loss: 0.1172\n",
            "\n",
            "avg valid loss: 0.1081\n",
            "learning rate: [0.0001]\n",
            "valid loss did not improved for 1 epochs...\n",
            "[epoch 13, batch  1000] loss 0.1160\n",
            "[epoch 13, batch  2000] loss 0.1164\n",
            "[epoch 13, batch  3000] loss 0.1164\n",
            "[epoch 13, batch  4000] loss 0.1164\n",
            "[epoch 13, batch  5000] loss 0.1166\n",
            "[epoch 13, batch  6000] loss 0.1165\n",
            "[epoch 13, batch  7000] loss 0.1166\n",
            "[epoch 13, batch  8000] loss 0.1166\n",
            "[epoch 13, batch  9000] loss 0.1167\n",
            "[epoch 13, batch 10000] loss 0.1168\n",
            "[epoch 13, batch 11000] loss 0.1169\n",
            "[epoch 13, batch 12000] loss 0.1169\n",
            "[epoch 13, batch 13000] loss 0.1169\n",
            "[epoch 13, batch 14000] loss 0.1169\n",
            "[epoch 13, batch 15000] loss 0.1169\n",
            ">>> epoch 13 finished in 75.76 seconds...\n",
            "\n",
            "avg training loss: 0.1169\n",
            "\n",
            "avg valid loss: 0.1075\n",
            "learning rate: [0.0001]\n",
            "valid loss improved, saving model /content/drive/MyDrive/chess_games/sf_chess_model.pth\n",
            "[epoch 14, batch  1000] loss 0.1170\n",
            "[epoch 14, batch  2000] loss 0.1166\n",
            "[epoch 14, batch  3000] loss 0.1164\n",
            "[epoch 14, batch  4000] loss 0.1165\n",
            "[epoch 14, batch  5000] loss 0.1164\n",
            "[epoch 14, batch  6000] loss 0.1164\n",
            "[epoch 14, batch  7000] loss 0.1164\n",
            "[epoch 14, batch  8000] loss 0.1165\n",
            "[epoch 14, batch  9000] loss 0.1166\n",
            "[epoch 14, batch 10000] loss 0.1166\n",
            "[epoch 14, batch 11000] loss 0.1166\n",
            "[epoch 14, batch 12000] loss 0.1167\n",
            "[epoch 14, batch 13000] loss 0.1167\n",
            "[epoch 14, batch 14000] loss 0.1167\n",
            "[epoch 14, batch 15000] loss 0.1166\n",
            ">>> epoch 14 finished in 75.76 seconds...\n",
            "\n",
            "avg training loss: 0.1166\n",
            "\n",
            "avg valid loss: 0.1073\n",
            "learning rate: [0.0001]\n",
            "valid loss improved, saving model /content/drive/MyDrive/chess_games/sf_chess_model.pth\n",
            "[epoch 15, batch  1000] loss 0.1161\n",
            "[epoch 15, batch  2000] loss 0.1163\n",
            "[epoch 15, batch  3000] loss 0.1161\n",
            "[epoch 15, batch  4000] loss 0.1163\n",
            "[epoch 15, batch  5000] loss 0.1162\n",
            "[epoch 15, batch  6000] loss 0.1161\n",
            "[epoch 15, batch  7000] loss 0.1160\n",
            "[epoch 15, batch  8000] loss 0.1160\n",
            "[epoch 15, batch  9000] loss 0.1160\n",
            "[epoch 15, batch 10000] loss 0.1161\n",
            "[epoch 15, batch 11000] loss 0.1161\n",
            "[epoch 15, batch 12000] loss 0.1162\n",
            "[epoch 15, batch 13000] loss 0.1162\n",
            "[epoch 15, batch 14000] loss 0.1162\n",
            "[epoch 15, batch 15000] loss 0.1163\n",
            ">>> epoch 15 finished in 75.31 seconds...\n",
            "\n",
            "avg training loss: 0.1163\n",
            "\n",
            "avg valid loss: 0.1076\n",
            "learning rate: [0.0001]\n",
            "valid loss did not improved for 1 epochs...\n",
            "[epoch 16, batch  1000] loss 0.1164\n",
            "[epoch 16, batch  2000] loss 0.1160\n",
            "[epoch 16, batch  3000] loss 0.1159\n",
            "[epoch 16, batch  4000] loss 0.1157\n",
            "[epoch 16, batch  5000] loss 0.1158\n",
            "[epoch 16, batch  6000] loss 0.1159\n",
            "[epoch 16, batch  7000] loss 0.1159\n",
            "[epoch 16, batch  8000] loss 0.1159\n",
            "[epoch 16, batch  9000] loss 0.1158\n",
            "[epoch 16, batch 10000] loss 0.1159\n",
            "[epoch 16, batch 11000] loss 0.1158\n",
            "[epoch 16, batch 12000] loss 0.1158\n",
            "[epoch 16, batch 13000] loss 0.1159\n",
            "[epoch 16, batch 14000] loss 0.1160\n",
            "[epoch 16, batch 15000] loss 0.1159\n",
            ">>> epoch 16 finished in 75.41 seconds...\n",
            "\n",
            "avg training loss: 0.1159\n",
            "\n",
            "avg valid loss: 0.1070\n",
            "learning rate: [0.0001]\n",
            "valid loss improved, saving model /content/drive/MyDrive/chess_games/sf_chess_model.pth\n",
            "[epoch 17, batch  1000] loss 0.1145\n",
            "[epoch 17, batch  2000] loss 0.1150\n",
            "[epoch 17, batch  3000] loss 0.1153\n",
            "[epoch 17, batch  4000] loss 0.1156\n",
            "[epoch 17, batch  5000] loss 0.1156\n",
            "[epoch 17, batch  6000] loss 0.1156\n",
            "[epoch 17, batch  7000] loss 0.1155\n",
            "[epoch 17, batch  8000] loss 0.1155\n",
            "[epoch 17, batch  9000] loss 0.1155\n",
            "[epoch 17, batch 10000] loss 0.1157\n",
            "[epoch 17, batch 11000] loss 0.1158\n",
            "[epoch 17, batch 12000] loss 0.1157\n",
            "[epoch 17, batch 13000] loss 0.1157\n",
            "[epoch 17, batch 14000] loss 0.1157\n",
            "[epoch 17, batch 15000] loss 0.1157\n",
            ">>> epoch 17 finished in 75.47 seconds...\n",
            "\n",
            "avg training loss: 0.1157\n",
            "\n",
            "avg valid loss: 0.1069\n",
            "learning rate: [0.0001]\n",
            "valid loss improved, saving model /content/drive/MyDrive/chess_games/sf_chess_model.pth\n",
            "[epoch 18, batch  1000] loss 0.1150\n",
            "[epoch 18, batch  2000] loss 0.1147\n",
            "[epoch 18, batch  3000] loss 0.1149\n",
            "[epoch 18, batch  4000] loss 0.1150\n",
            "[epoch 18, batch  5000] loss 0.1149\n",
            "[epoch 18, batch  6000] loss 0.1150\n",
            "[epoch 18, batch  7000] loss 0.1151\n",
            "[epoch 18, batch  8000] loss 0.1152\n",
            "[epoch 18, batch  9000] loss 0.1152\n",
            "[epoch 18, batch 10000] loss 0.1152\n",
            "[epoch 18, batch 11000] loss 0.1152\n",
            "[epoch 18, batch 12000] loss 0.1152\n",
            "[epoch 18, batch 13000] loss 0.1153\n",
            "[epoch 18, batch 14000] loss 0.1153\n",
            "[epoch 18, batch 15000] loss 0.1153\n",
            ">>> epoch 18 finished in 75.58 seconds...\n",
            "\n",
            "avg training loss: 0.1154\n",
            "\n",
            "avg valid loss: 0.1073\n",
            "learning rate: [0.0001]\n",
            "valid loss did not improved for 1 epochs...\n",
            "[epoch 19, batch  1000] loss 0.1143\n",
            "[epoch 19, batch  2000] loss 0.1143\n",
            "[epoch 19, batch  3000] loss 0.1145\n",
            "[epoch 19, batch  4000] loss 0.1146\n",
            "[epoch 19, batch  5000] loss 0.1148\n",
            "[epoch 19, batch  6000] loss 0.1148\n",
            "[epoch 19, batch  7000] loss 0.1149\n",
            "[epoch 19, batch  8000] loss 0.1149\n",
            "[epoch 19, batch  9000] loss 0.1150\n",
            "[epoch 19, batch 10000] loss 0.1150\n",
            "[epoch 19, batch 11000] loss 0.1150\n",
            "[epoch 19, batch 12000] loss 0.1150\n",
            "[epoch 19, batch 13000] loss 0.1150\n",
            "[epoch 19, batch 14000] loss 0.1151\n",
            "[epoch 19, batch 15000] loss 0.1150\n",
            ">>> epoch 19 finished in 75.37 seconds...\n",
            "\n",
            "avg training loss: 0.1150\n",
            "\n",
            "avg valid loss: 0.1067\n",
            "learning rate: [0.0001]\n",
            "valid loss improved, saving model /content/drive/MyDrive/chess_games/sf_chess_model.pth\n",
            "[epoch 20, batch  1000] loss 0.1140\n",
            "[epoch 20, batch  2000] loss 0.1145\n",
            "[epoch 20, batch  3000] loss 0.1141\n",
            "[epoch 20, batch  4000] loss 0.1144\n",
            "[epoch 20, batch  5000] loss 0.1144\n",
            "[epoch 20, batch  6000] loss 0.1144\n",
            "[epoch 20, batch  7000] loss 0.1144\n",
            "[epoch 20, batch  8000] loss 0.1144\n",
            "[epoch 20, batch  9000] loss 0.1144\n",
            "[epoch 20, batch 10000] loss 0.1146\n",
            "[epoch 20, batch 11000] loss 0.1146\n",
            "[epoch 20, batch 12000] loss 0.1146\n",
            "[epoch 20, batch 13000] loss 0.1146\n",
            "[epoch 20, batch 14000] loss 0.1147\n",
            "[epoch 20, batch 15000] loss 0.1147\n",
            ">>> epoch 20 finished in 75.41 seconds...\n",
            "\n",
            "avg training loss: 0.1147\n",
            "\n",
            "avg valid loss: 0.1066\n",
            "learning rate: [0.0001]\n",
            "valid loss improved, saving model /content/drive/MyDrive/chess_games/sf_chess_model.pth\n",
            "[epoch 21, batch  1000] loss 0.1132\n",
            "[epoch 21, batch  2000] loss 0.1136\n",
            "[epoch 21, batch  3000] loss 0.1137\n",
            "[epoch 21, batch  4000] loss 0.1138\n",
            "[epoch 21, batch  5000] loss 0.1139\n",
            "[epoch 21, batch  6000] loss 0.1139\n",
            "[epoch 21, batch  7000] loss 0.1140\n",
            "[epoch 21, batch  8000] loss 0.1141\n",
            "[epoch 21, batch  9000] loss 0.1142\n",
            "[epoch 21, batch 10000] loss 0.1143\n",
            "[epoch 21, batch 11000] loss 0.1143\n",
            "[epoch 21, batch 12000] loss 0.1144\n",
            "[epoch 21, batch 13000] loss 0.1144\n",
            "[epoch 21, batch 14000] loss 0.1145\n",
            "[epoch 21, batch 15000] loss 0.1145\n",
            ">>> epoch 21 finished in 75.33 seconds...\n",
            "\n",
            "avg training loss: 0.1145\n",
            "\n",
            "avg valid loss: 0.1071\n",
            "learning rate: [0.0001]\n",
            "valid loss did not improved for 1 epochs...\n",
            "[epoch 22, batch  1000] loss 0.1135\n",
            "[epoch 22, batch  2000] loss 0.1139\n",
            "[epoch 22, batch  3000] loss 0.1140\n",
            "[epoch 22, batch  4000] loss 0.1138\n",
            "[epoch 22, batch  5000] loss 0.1137\n",
            "[epoch 22, batch  6000] loss 0.1138\n",
            "[epoch 22, batch  7000] loss 0.1139\n",
            "[epoch 22, batch  8000] loss 0.1140\n",
            "[epoch 22, batch  9000] loss 0.1140\n",
            "[epoch 22, batch 10000] loss 0.1141\n",
            "[epoch 22, batch 11000] loss 0.1142\n",
            "[epoch 22, batch 12000] loss 0.1142\n",
            "[epoch 22, batch 13000] loss 0.1142\n",
            "[epoch 22, batch 14000] loss 0.1142\n",
            "[epoch 22, batch 15000] loss 0.1142\n",
            ">>> epoch 22 finished in 75.39 seconds...\n",
            "\n",
            "avg training loss: 0.1142\n",
            "\n",
            "avg valid loss: 0.1068\n",
            "learning rate: [0.0001]\n",
            "valid loss did not improved for 2 epochs...\n",
            "[epoch 23, batch  1000] loss 0.1134\n",
            "[epoch 23, batch  2000] loss 0.1133\n",
            "[epoch 23, batch  3000] loss 0.1134\n",
            "[epoch 23, batch  4000] loss 0.1137\n",
            "[epoch 23, batch  5000] loss 0.1139\n",
            "[epoch 23, batch  6000] loss 0.1140\n",
            "[epoch 23, batch  7000] loss 0.1140\n",
            "[epoch 23, batch  8000] loss 0.1139\n",
            "[epoch 23, batch  9000] loss 0.1140\n",
            "[epoch 23, batch 10000] loss 0.1139\n",
            "[epoch 23, batch 11000] loss 0.1139\n",
            "[epoch 23, batch 12000] loss 0.1138\n",
            "[epoch 23, batch 13000] loss 0.1138\n",
            "[epoch 23, batch 14000] loss 0.1139\n",
            "[epoch 23, batch 15000] loss 0.1140\n",
            ">>> epoch 23 finished in 75.43 seconds...\n",
            "\n",
            "avg training loss: 0.1140\n",
            "\n",
            "avg valid loss: 0.1072\n",
            "learning rate: [0.0001]\n",
            "valid loss did not improved for 3 epochs...\n",
            "[epoch 24, batch  1000] loss 0.1130\n",
            "[epoch 24, batch  2000] loss 0.1137\n",
            "[epoch 24, batch  3000] loss 0.1135\n",
            "[epoch 24, batch  4000] loss 0.1135\n",
            "[epoch 24, batch  5000] loss 0.1134\n",
            "[epoch 24, batch  6000] loss 0.1135\n",
            "[epoch 24, batch  7000] loss 0.1134\n",
            "[epoch 24, batch  8000] loss 0.1136\n",
            "[epoch 24, batch  9000] loss 0.1136\n",
            "[epoch 24, batch 10000] loss 0.1136\n",
            "[epoch 24, batch 11000] loss 0.1136\n",
            "[epoch 24, batch 12000] loss 0.1136\n",
            "[epoch 24, batch 13000] loss 0.1136\n",
            "[epoch 24, batch 14000] loss 0.1137\n",
            "[epoch 24, batch 15000] loss 0.1137\n",
            ">>> epoch 24 finished in 75.37 seconds...\n",
            "\n",
            "avg training loss: 0.1137\n",
            "\n",
            "avg valid loss: 0.1074\n",
            "learning rate: [1e-05]\n",
            "valid loss did not improved for 4 epochs...\n",
            "[epoch 25, batch  1000] loss 0.1122\n",
            "[epoch 25, batch  2000] loss 0.1119\n",
            "[epoch 25, batch  3000] loss 0.1118\n",
            "[epoch 25, batch  4000] loss 0.1118\n",
            "[epoch 25, batch  5000] loss 0.1119\n",
            "[epoch 25, batch  6000] loss 0.1118\n",
            "[epoch 25, batch  7000] loss 0.1120\n",
            "[epoch 25, batch  8000] loss 0.1120\n",
            "[epoch 25, batch  9000] loss 0.1119\n",
            "[epoch 25, batch 10000] loss 0.1120\n",
            "[epoch 25, batch 11000] loss 0.1120\n",
            "[epoch 25, batch 12000] loss 0.1121\n",
            "[epoch 25, batch 13000] loss 0.1121\n",
            "[epoch 25, batch 14000] loss 0.1121\n",
            "[epoch 25, batch 15000] loss 0.1120\n",
            ">>> epoch 25 finished in 75.49 seconds...\n",
            "\n",
            "avg training loss: 0.1120\n",
            "\n",
            "avg valid loss: 0.1069\n",
            "learning rate: [1e-05]\n",
            "valid loss did not improved for 5 epochs...\n",
            "[epoch 26, batch  1000] loss 0.1111\n",
            "[epoch 26, batch  2000] loss 0.1121\n",
            "[epoch 26, batch  3000] loss 0.1119\n",
            "[epoch 26, batch  4000] loss 0.1117\n",
            "[epoch 26, batch  5000] loss 0.1116\n",
            "[epoch 26, batch  6000] loss 0.1115\n",
            "[epoch 26, batch  7000] loss 0.1116\n",
            "[epoch 26, batch  8000] loss 0.1115\n",
            "[epoch 26, batch  9000] loss 0.1116\n",
            "[epoch 26, batch 10000] loss 0.1115\n",
            "[epoch 26, batch 11000] loss 0.1116\n",
            "[epoch 26, batch 12000] loss 0.1116\n",
            "[epoch 26, batch 13000] loss 0.1116\n",
            "[epoch 26, batch 14000] loss 0.1116\n",
            "[epoch 26, batch 15000] loss 0.1116\n",
            ">>> epoch 26 finished in 75.64 seconds...\n",
            "\n",
            "avg training loss: 0.1116\n",
            "\n",
            "avg valid loss: 0.1071\n",
            "learning rate: [1e-05]\n",
            "valid loss did not improved for 6 epochs...\n",
            "[epoch 27, batch  1000] loss 0.1113\n",
            "[epoch 27, batch  2000] loss 0.1116\n",
            "[epoch 27, batch  3000] loss 0.1113\n",
            "[epoch 27, batch  4000] loss 0.1114\n",
            "[epoch 27, batch  5000] loss 0.1114\n",
            "[epoch 27, batch  6000] loss 0.1116\n",
            "[epoch 27, batch  7000] loss 0.1115\n",
            "[epoch 27, batch  8000] loss 0.1114\n",
            "[epoch 27, batch  9000] loss 0.1114\n",
            "[epoch 27, batch 10000] loss 0.1114\n",
            "[epoch 27, batch 11000] loss 0.1114\n",
            "[epoch 27, batch 12000] loss 0.1113\n",
            "[epoch 27, batch 13000] loss 0.1114\n",
            "[epoch 27, batch 14000] loss 0.1114\n",
            "[epoch 27, batch 15000] loss 0.1113\n",
            ">>> epoch 27 finished in 75.56 seconds...\n",
            "\n",
            "avg training loss: 0.1113\n",
            "\n",
            "avg valid loss: 0.1073\n",
            "learning rate: [1e-05]\n",
            "valid loss did not improved for 7 epochs...\n",
            "[epoch 28, batch  1000] loss 0.1118\n",
            "[epoch 28, batch  2000] loss 0.1113\n",
            "[epoch 28, batch  3000] loss 0.1109\n",
            "[epoch 28, batch  4000] loss 0.1109\n",
            "[epoch 28, batch  5000] loss 0.1109\n",
            "[epoch 28, batch  6000] loss 0.1111\n",
            "[epoch 28, batch  7000] loss 0.1110\n",
            "[epoch 28, batch  8000] loss 0.1110\n",
            "[epoch 28, batch  9000] loss 0.1111\n",
            "[epoch 28, batch 10000] loss 0.1111\n",
            "[epoch 28, batch 11000] loss 0.1111\n",
            "[epoch 28, batch 12000] loss 0.1111\n",
            "[epoch 28, batch 13000] loss 0.1111\n",
            "[epoch 28, batch 14000] loss 0.1111\n",
            "[epoch 28, batch 15000] loss 0.1111\n",
            ">>> epoch 28 finished in 75.57 seconds...\n",
            "\n",
            "avg training loss: 0.1111\n",
            "\n",
            "avg valid loss: 0.1073\n",
            "learning rate: [1.0000000000000002e-06]\n",
            "valid loss did not improved for 8 epochs...\n",
            "[epoch 29, batch  1000] loss 0.1112\n",
            "[epoch 29, batch  2000] loss 0.1113\n",
            "[epoch 29, batch  3000] loss 0.1111\n",
            "[epoch 29, batch  4000] loss 0.1111\n",
            "[epoch 29, batch  5000] loss 0.1109\n",
            "[epoch 29, batch  6000] loss 0.1110\n",
            "[epoch 29, batch  7000] loss 0.1111\n",
            "[epoch 29, batch  8000] loss 0.1110\n",
            "[epoch 29, batch  9000] loss 0.1110\n",
            "[epoch 29, batch 10000] loss 0.1110\n",
            "[epoch 29, batch 11000] loss 0.1108\n",
            "[epoch 29, batch 12000] loss 0.1108\n",
            "[epoch 29, batch 13000] loss 0.1107\n",
            "[epoch 29, batch 14000] loss 0.1107\n",
            "[epoch 29, batch 15000] loss 0.1107\n",
            ">>> epoch 29 finished in 75.52 seconds...\n",
            "\n",
            "avg training loss: 0.1107\n",
            "\n",
            "avg valid loss: 0.1074\n",
            "learning rate: [1.0000000000000002e-06]\n",
            "valid loss did not improved for 9 epochs...\n",
            "[epoch 30, batch  1000] loss 0.1101\n",
            "[epoch 30, batch  2000] loss 0.1107\n",
            "[epoch 30, batch  3000] loss 0.1107\n",
            "[epoch 30, batch  4000] loss 0.1108\n",
            "[epoch 30, batch  5000] loss 0.1107\n",
            "[epoch 30, batch  6000] loss 0.1106\n",
            "[epoch 30, batch  7000] loss 0.1105\n",
            "[epoch 30, batch  8000] loss 0.1106\n",
            "[epoch 30, batch  9000] loss 0.1105\n",
            "[epoch 30, batch 10000] loss 0.1106\n",
            "[epoch 30, batch 11000] loss 0.1106\n",
            "[epoch 30, batch 12000] loss 0.1107\n",
            "[epoch 30, batch 13000] loss 0.1107\n",
            "[epoch 30, batch 14000] loss 0.1107\n",
            "[epoch 30, batch 15000] loss 0.1107\n",
            ">>> epoch 30 finished in 75.53 seconds...\n",
            "\n",
            "avg training loss: 0.1107\n",
            "\n",
            "avg valid loss: 0.1074\n",
            "learning rate: [1.0000000000000002e-06]\n",
            "valid loss did not improved for 10 epochs...\n",
            "early stop triggered, epoch: 30\n",
            "\n",
            "===== TRAINING COMPLETE\n",
            "total train time: 2277.50 seconds\n",
            "best val loss: 0.1066\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  print(\"loading train data...\")\n",
        "  train_dataset = ChessDataset(TRAIN_FILES)\n",
        "\n",
        "  print(\"\\nloading valid data...\")\n",
        "  valid_dataset = ChessDataset([VALID_FILE])\n",
        "\n",
        "  if len(train_dataset) > 0 and len(valid_dataset) > 0:\n",
        "    train_loader = DataLoader(train_dataset,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              shuffle=True,\n",
        "                              num_workers=0, # 2\n",
        "                              pin_memory=True) # true for faster gpu transfer\n",
        "    valid_loader = DataLoader(valid_dataset,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              shuffle=False,\n",
        "                              num_workers=0,\n",
        "                              pin_memory=True)\n",
        "\n",
        "    print(\"\\ndataset ready to train\")\n",
        "\n",
        "    # init model, loss and optimizer\n",
        "    model = ChessCNN().to(device)\n",
        "    criterion = nn.MSELoss() # mean squared error (sweet for regression)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    #scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
        "                                                     mode='min', # reduce lr when metric stop decreasinh\n",
        "                                                     factor=.1, # reduce lr by factor 10\n",
        "                                                     patience=3) # wait 3 epoch with no imrpovement before reducing\n",
        "\n",
        "    print(\"\\n===== TRAINING STARTS\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    best_valid_loss = float(\"inf\")\n",
        "    epochs_no_improve = 0\n",
        "    early_stopping_patience = 10\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "      epoch_start_time = time.time()\n",
        "      model.train()\n",
        "\n",
        "      running_train_loss = 0.0\n",
        "      num_train_batches = 0\n",
        "\n",
        "\n",
        "      for i, (boards, labels) in enumerate(train_loader):\n",
        "        # move data to gpu\n",
        "        boards = boards.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(boards)\n",
        "\n",
        "        # calculate loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_train_loss += loss.item()\n",
        "        num_train_batches += 1\n",
        "\n",
        "        if (i + 1) % 1000 == 0:\n",
        "          # 2.16m positions, 2.16m/128 ~ 16875 batches per epoch\n",
        "          avg_train_loss_so_far = running_train_loss / (i + 1)\n",
        "          print(f\"[epoch {epoch + 1}, batch {i + 1:5d}] loss {avg_train_loss_so_far:.4f}\")\n",
        "\n",
        "      avg_train_loss = running_train_loss / num_train_batches\n",
        "\n",
        "      model.eval()\n",
        "      running_valid_loss = 0.0\n",
        "      num_valid_batches = 0\n",
        "\n",
        "      with torch.no_grad():\n",
        "        for boards, labels in valid_loader:\n",
        "          boards = boards.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          outputs = model(boards)\n",
        "          loss = criterion(outputs, labels)\n",
        "\n",
        "          running_valid_loss += loss.item()\n",
        "          num_valid_batches += 1\n",
        "\n",
        "      avg_valid_loss = running_valid_loss / num_valid_batches\n",
        "\n",
        "      epoch_time = time.time() - epoch_start_time\n",
        "      print(f\">>> epoch {epoch + 1} finished in {epoch_time:.2f} seconds...\")\n",
        "      print(f\"\\navg training loss: {avg_train_loss:.4f}\")\n",
        "      print(f\"\\navg valid loss: {avg_valid_loss:.4f}\")\n",
        "\n",
        "      scheduler.step(avg_valid_loss) # update learning rate\n",
        "      print(f\"learning rate: {scheduler.get_last_lr()}\")\n",
        "\n",
        "      if avg_valid_loss < best_valid_loss:\n",
        "        best_valid_loss = avg_valid_loss\n",
        "        epochs_no_improve = 0\n",
        "        print(f\"valid loss improved, saving model {MODEL_SAVE_PATH}\")\n",
        "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "      else:\n",
        "        epochs_no_improve += 1\n",
        "        print(f\"valid loss did not improved for {epochs_no_improve} epochs...\")\n",
        "        if epochs_no_improve >= early_stopping_patience:\n",
        "          print(f\"early stop triggered, epoch: {epoch + 1}\")\n",
        "          break\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\n===== TRAINING COMPLETE\")\n",
        "    print(f\"total train time: {total_time:.2f} seconds\")\n",
        "    print(f\"best val loss: {best_valid_loss:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "  print(f\"\\n ERROR during training {e}\")\n",
        "  import traceback\n",
        "  traceback.print_exc()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
